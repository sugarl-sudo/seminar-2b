model:
  num_encoder_layers: 6
  num_encoder_heads: 8
  num_decoder_layers: 6
  num_decoder_heads: 8
  d_model: 512
  encoder_ffn_dim: 2048
  decoder_ffn_dim: 2048
  max_sequence_length: 512

train:
  output_dir: results/relu-inv/n=10
  num_train_epochs: 10
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_ratio: 0.1
  batch_size: 128
  test_batch_size: 128
  lr_scheduler_type: "linear"
  max_grad_norm: 1.0
  optimizer: adamw_torch
  num_workers: 4
  seed: 42
  dryrun: False

data:
  train_dataset_path: data/relu/n=10/data-inv.train
  test_dataset_path: data/relu/n=10/data-inv.test
  num_variables: 0
  max_degree: 20
  max_coeff: 500
  field: ZZ
  num_train_samples: -1
  num_test_samples: -1
  processor_name: plain

wandb:
  project: seminar-2b
  group: relu-inverse
  name: n=10-inv
  no_wandb: False

